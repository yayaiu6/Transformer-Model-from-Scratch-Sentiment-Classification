{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d21142e4",
   "metadata": {},
   "source": [
    "# Transformer Model from Scratch for Sentiment Classification\n",
    "\n",
    "This notebook demonstrates how to build a Transformer model from scratch for sentiment classification using the Sentiment140 dataset. The model architecture includes Rotary Positional Encoding (RoPE), Multi-Head Attention, RMSNorm, and a SwiGLU FeedForward network. The notebook covers all steps: data preparation, model definition, training with early stopping, evaluation, and recommendations for further improvement. Detailed explanations and code for each component are provided in the following sections."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c49291cf",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-03T14:05:55.392220Z",
     "iopub.status.busy": "2025-05-03T14:05:55.391645Z",
     "iopub.status.idle": "2025-05-03T14:06:00.976367Z",
     "shell.execute_reply": "2025-05-03T14:06:00.975514Z",
     "shell.execute_reply.started": "2025-05-03T14:05:55.392196Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, TensorDataset, random_split\n",
    "from torch.optim import AdamW\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
    "from transformers import BertTokenizer\n",
    "from datasets import load_dataset\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import f1_score\n",
    "import math\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "474912e1",
   "metadata": {},
   "source": [
    "## Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee972ede",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-03T14:06:00.978397Z",
     "iopub.status.busy": "2025-05-03T14:06:00.977902Z",
     "iopub.status.idle": "2025-05-03T14:17:42.983926Z",
     "shell.execute_reply": "2025-05-03T14:17:42.983167Z",
     "shell.execute_reply.started": "2025-05-03T14:06:00.978374Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e84c62fd4ec243babdc63ed9d7d7166f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/6.84k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "062b68dfc7a34520a07dc4da5c1236ac",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "sentiment140.py:   0%|          | 0.00/4.03k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "017816614675489d95407fd1632aa82b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/81.4M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6a0f5aeb918a4e349ddf2693af83f26a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/1600000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "77c96832b26546e28494f6de2f5324d8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test split:   0%|          | 0/498 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a036dae64348414fbd7c51c97945f59c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/49.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ae699db7cb06492c8a1b0c115634e0fb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/996k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4e0ce5d52f8847deb609a5c4ee747494",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.96M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1d259594a9154ef6acd4451cee9e5a5f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/625 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5516d284079747608cfc440fdfae87df",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1600000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "af01ecfc6939480e824dd230e5713cf6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1600000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training samples: 1280000, Validation samples: 320000\n"
     ]
    }
   ],
   "source": [
    "\n",
    "dataset = load_dataset(\"sentiment140\", split=\"train\", trust_remote_code=True)\n",
    "\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-multilingual-cased')\n",
    "\n",
    "\n",
    "def tokenize_function(example):\n",
    "    return tokenizer(example['text'], padding='max_length', truncation=True, max_length=64)\n",
    "\n",
    "\n",
    "def process_labels(example):\n",
    "    \n",
    "    if example['sentiment'] == 0:\n",
    "        example['label'] = 0  # Negative\n",
    "    elif example['sentiment'] == 2:\n",
    "        example['label'] = 1  # Neutral\n",
    "    elif example['sentiment'] == 4:\n",
    "        example['label'] = 2  # Positive\n",
    "    return example\n",
    "\n",
    "\n",
    "dataset = dataset.map(process_labels)\n",
    "tokenized_dataset = dataset.map(tokenize_function, batched=True)\n",
    "\n",
    "\n",
    "input_ids = torch.tensor(tokenized_dataset['input_ids'], dtype=torch.long)\n",
    "attention_mask = torch.tensor(tokenized_dataset['attention_mask'], dtype=torch.long)\n",
    "labels = torch.tensor(tokenized_dataset['label'], dtype=torch.long)\n",
    "\n",
    "\n",
    "full_dataset = TensorDataset(input_ids, attention_mask, labels)\n",
    "\n",
    "\n",
    "train_size = int(0.8 * len(full_dataset))\n",
    "val_size = len(full_dataset) - train_size\n",
    "train_dataset, val_dataset = random_split(full_dataset, [train_size, val_size])\n",
    "\n",
    "\n",
    "batch_size = 16\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "print(f\"Training samples: {len(train_dataset)}, Validation samples: {len(val_dataset)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c6b4e2b",
   "metadata": {},
   "source": [
    "## Input Embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd6df716",
   "metadata": {},
   "source": [
    "### Rotary Positional Encoding (RoPE)\n",
    "\n",
    "Rotary Positional Encoding (RoPE) enables the model to capture relative and absolute positional information efficiently, improving the performance of attention mechanisms in Transformers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e5e4946",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-03T14:17:42.984994Z",
     "iopub.status.busy": "2025-05-03T14:17:42.984726Z",
     "iopub.status.idle": "2025-05-03T14:17:43.003450Z",
     "shell.execute_reply": "2025-05-03T14:17:43.002803Z",
     "shell.execute_reply.started": "2025-05-03T14:17:42.984969Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class Rotary_Positional_Encoding:\n",
    "    def __init__(self, dim, device):\n",
    "        assert dim % 2 == 0\n",
    "        self.dim = dim\n",
    "        self.inv_freq = 1.0 / (10000 ** (torch.arange(0, dim, 2).float() / dim))  \n",
    "\n",
    "    def get_position_angles(self, seq_len, device):\n",
    "        positions = torch.arange(seq_len, dtype=torch.float, device=device)\n",
    "        freqs = torch.einsum(\"i,j->ij\", positions, self.inv_freq.to(device))  \n",
    "        return torch.cat((freqs.sin(), freqs.cos()), dim=-1)\n",
    "\n",
    "    def apply_rotary(self, x, seq_len=None):\n",
    "        bsz, seqlen, dim = x.shape\n",
    "        assert dim == self.dim\n",
    "        if seq_len is None:\n",
    "            seq_len = seqlen\n",
    "\n",
    "        x1 = x[..., ::2]\n",
    "        x2 = x[..., 1::2]\n",
    "\n",
    "        freqs = self.get_position_angles(seq_len, x.device).unsqueeze(0)\n",
    "        sin = freqs[..., :self.dim // 2]\n",
    "        cos = freqs[..., self.dim // 2:]\n",
    "\n",
    "        x_rotated_even = x1 * cos - x2 * sin\n",
    "        x_rotated_odd = x1 * sin + x2 * cos\n",
    "\n",
    "        x_rotated = torch.stack((x_rotated_even, x_rotated_odd), dim=-1)\n",
    "        return x_rotated.flatten(-2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7aef16e",
   "metadata": {},
   "source": [
    "### Sentence Embedding\n",
    "\n",
    "combines token embeddings with positional information, allowing the model to understand both the meaning of words and their order in the sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "20698128",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-03T14:17:43.005229Z",
     "iopub.status.busy": "2025-05-03T14:17:43.005002Z",
     "iopub.status.idle": "2025-05-03T14:17:43.026353Z",
     "shell.execute_reply": "2025-05-03T14:17:43.025471Z",
     "shell.execute_reply.started": "2025-05-03T14:17:43.005212Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class SentenceEmbedding(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim=256, device='cpu'):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_dim)\n",
    "        self.pos_encoding = Rotary_Positional_Encoding(embed_dim, device)\n",
    "\n",
    "    def forward(self, x):\n",
    "        embedded = self.embedding(x)\n",
    "        return self.pos_encoding.apply_rotary(embedded, x.size(1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "409c374f",
   "metadata": {},
   "source": [
    "## Multi-Head Attention"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8de06ceb",
   "metadata": {},
   "source": [
    "### Scaled Dot-Product Attention\n",
    "\n",
    "This function computes the attention weights and output for the attention mechanism, allowing the model to focus on relevant parts of the input sequence when making predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4bf065c7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-03T14:17:43.027652Z",
     "iopub.status.busy": "2025-05-03T14:17:43.027347Z",
     "iopub.status.idle": "2025-05-03T14:17:43.043521Z",
     "shell.execute_reply": "2025-05-03T14:17:43.042933Z",
     "shell.execute_reply.started": "2025-05-03T14:17:43.027624Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def scaled_dot_product_attention(q, k, v, mask=None):\n",
    "    d_k = q.size(-1)\n",
    "    scores = torch.matmul(q, k.transpose(-2, -1)) / math.sqrt(d_k)\n",
    "    if mask is not None:\n",
    "        mask = mask.unsqueeze(1).unsqueeze(2)  \n",
    "        mask = mask.expand(-1, -1, mask.size(-1), -1)  \n",
    "        scores = scores.masked_fill(mask == 0, -1e9)\n",
    "    attn = torch.softmax(scores, dim=-1)\n",
    "    output = torch.matmul(attn, v)\n",
    "    return output, attn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2a93115",
   "metadata": {},
   "source": [
    "### Multi-Head Attention\n",
    "\n",
    "Multi_Head_Attention allows the model to attend to information from different representation subspaces at different positions, improving its ability to capture complex relationships in the input sequence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ed75f198",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-03T14:17:43.044296Z",
     "iopub.status.busy": "2025-05-03T14:17:43.044060Z",
     "iopub.status.idle": "2025-05-03T14:17:43.060271Z",
     "shell.execute_reply": "2025-05-03T14:17:43.059357Z",
     "shell.execute_reply.started": "2025-05-03T14:17:43.044280Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class Multi_Head_Attention(nn.Module):\n",
    "    def __init__(self, d_model, num_heads):\n",
    "        super().__init__()\n",
    "        self.num_heads = num_heads\n",
    "        self.d_k = d_model // num_heads\n",
    "\n",
    "        self.q_linear = nn.Linear(d_model, d_model)\n",
    "        self.k_linear = nn.Linear(d_model, d_model)\n",
    "        self.v_linear = nn.Linear(d_model, d_model)\n",
    "\n",
    "        self.out_linear = nn.Linear(d_model, d_model)\n",
    "\n",
    "    def forward(self, q, k, v, mask=None):\n",
    "        batch_size = q.size(0)\n",
    "\n",
    "        q = self.q_linear(q).view(batch_size, -1, self.num_heads, self.d_k).transpose(1, 2)\n",
    "        k = self.k_linear(k).view(batch_size, -1, self.num_heads, self.d_k).transpose(1, 2)\n",
    "        v = self.v_linear(v).view(batch_size, -1, self.num_heads, self.d_k).transpose(1, 2)\n",
    "\n",
    "        output, _ = scaled_dot_product_attention(q, k, v, mask)\n",
    "        output = output.transpose(1, 2).contiguous().view(batch_size, -1, self.num_heads * self.d_k)\n",
    "        return self.out_linear(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f0ee12b",
   "metadata": {},
   "source": [
    "## Add & Norm (RMSNorm)\n",
    "\n",
    "RMSNorm_Add applies Root Mean Square Layer Normalization and residual connection, which helps stabilize training and improve convergence in deep transformer models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9c31db04",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-03T14:17:43.061522Z",
     "iopub.status.busy": "2025-05-03T14:17:43.061249Z",
     "iopub.status.idle": "2025-05-03T14:17:43.077746Z",
     "shell.execute_reply": "2025-05-03T14:17:43.076927Z",
     "shell.execute_reply.started": "2025-05-03T14:17:43.061496Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class RMSNorm_Add(nn.Module):\n",
    "    def __init__(self, d_model, eps=1e-6, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.eps = eps\n",
    "        self.scale = nn.Parameter(torch.ones(d_model))\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x, sublayer_output):\n",
    "        residual = x + self.dropout(sublayer_output)\n",
    "        rms = torch.sqrt(torch.mean(residual ** 2, dim=-1, keepdim=True) + self.eps)\n",
    "        normalized = residual / rms\n",
    "        output = self.scale * normalized\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1677d3f9",
   "metadata": {},
   "source": [
    "## FeedForward (SwiGLU)\n",
    "\n",
    "FeedForward uses the SwiGLU activation to enhance the model's capacity to capture complex patterns, providing non-linearity and improving the expressiveness of each transformer block."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "62d5dc30",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-03T14:17:43.079651Z",
     "iopub.status.busy": "2025-05-03T14:17:43.078901Z",
     "iopub.status.idle": "2025-05-03T14:17:43.092565Z",
     "shell.execute_reply": "2025-05-03T14:17:43.091798Z",
     "shell.execute_reply.started": "2025-05-03T14:17:43.079622Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, d_model, d_ff, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.d_ff = d_ff\n",
    "        self.linear1 = nn.Linear(d_model, d_ff)\n",
    "        self.linear2 = nn.Linear(d_model, d_ff)\n",
    "        self.linear_out = nn.Linear(d_ff, d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        gate = torch.sigmoid(self.linear2(x))\n",
    "        x = self.linear1(x) * gate\n",
    "        x = self.dropout(x)\n",
    "        x = self.linear_out(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51cbd219",
   "metadata": {},
   "source": [
    "## Transformer Block\n",
    "\n",
    "TransformerBlock combines multi-head attention, normalization, and feedforward layers, enabling the model to learn complex dependencies and representations in the input data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9332e4ef",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-03T14:17:43.093644Z",
     "iopub.status.busy": "2025-05-03T14:17:43.093359Z",
     "iopub.status.idle": "2025-05-03T14:17:43.119187Z",
     "shell.execute_reply": "2025-05-03T14:17:43.118404Z",
     "shell.execute_reply.started": "2025-05-03T14:17:43.093610Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, d_model, num_heads, d_ff, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.attention = Multi_Head_Attention(d_model, num_heads)\n",
    "        self.norm1 = RMSNorm_Add(d_model, dropout=dropout)\n",
    "        self.feed_forward = FeedForward(d_model, d_ff, dropout=dropout)\n",
    "        self.norm2 = RMSNorm_Add(d_model, dropout=dropout)\n",
    "\n",
    "    def forward(self, x, mask=None):\n",
    "        attn_output = self.attention(x, x, x, mask)\n",
    "        x = self.norm1(x, attn_output)\n",
    "        ff_output = self.feed_forward(x)\n",
    "        x = self.norm2(x, ff_output)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be3205fa",
   "metadata": {},
   "source": [
    "## Transformer Model\n",
    "\n",
    "TransformerModel stacks embedding, multiple transformer blocks, and a classifier to process input sequences and perform sentiment classification end-to-end."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "297373e1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-03T14:17:43.122428Z",
     "iopub.status.busy": "2025-05-03T14:17:43.121746Z",
     "iopub.status.idle": "2025-05-03T14:17:43.137818Z",
     "shell.execute_reply": "2025-05-03T14:17:43.137136Z",
     "shell.execute_reply.started": "2025-05-03T14:17:43.122409Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class TransformerModel(nn.Module):\n",
    "    def __init__(self, vocab_size, d_model, num_heads, d_ff, num_layers, num_classes=3, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.embedding = SentenceEmbedding(vocab_size, d_model)\n",
    "        self.layers = nn.ModuleList([\n",
    "            TransformerBlock(d_model, num_heads, d_ff, dropout)\n",
    "            for _ in range(num_layers)\n",
    "        ])\n",
    "        self.classifier = nn.Linear(d_model, num_classes)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x, mask=None):\n",
    "        x = self.embedding(x)\n",
    "        for layer in self.layers:\n",
    "            x = layer(x, mask)\n",
    "        cls_output = x[:, 0, :]  \n",
    "        cls_output = self.dropout(cls_output)\n",
    "        logits = self.classifier(cls_output)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70f1a9ad",
   "metadata": {},
   "source": [
    "## Model Preparation\n",
    "\n",
    "Defining hyperparameters and initializing the model are essential steps to set up the architecture and prepare it for training and evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8931fdf7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-03T14:17:43.138844Z",
     "iopub.status.busy": "2025-05-03T14:17:43.138616Z",
     "iopub.status.idle": "2025-05-03T14:17:43.785502Z",
     "shell.execute_reply": "2025-05-03T14:17:43.784702Z",
     "shell.execute_reply.started": "2025-05-03T14:17:43.138825Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model initialized with 36919299 parameters\n"
     ]
    }
   ],
   "source": [
    "# Define hyperparameters\n",
    "vocab_size = tokenizer.vocab_size\n",
    "d_model = 256\n",
    "num_heads = 8\n",
    "d_ff = 1024\n",
    "num_layers = 6\n",
    "dropout = 0.3\n",
    "num_classes = 3  \n",
    "\n",
    "# Initialize model\n",
    "model = TransformerModel(\n",
    "    vocab_size=vocab_size,\n",
    "    d_model=d_model,\n",
    "    num_heads=num_heads,\n",
    "    d_ff=d_ff,\n",
    "    num_layers=num_layers,\n",
    "    num_classes=num_classes,\n",
    "    dropout=dropout\n",
    ").to(device)\n",
    "\n",
    "print(f\"Model initialized with {sum(p.numel() for p in model.parameters())} parameters\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40f09c32",
   "metadata": {},
   "source": [
    "## Training\n",
    "\n",
    "The training loop optimizes the model parameters, monitors validation performance, and applies early stopping to prevent overfitting, ensuring effective and efficient model training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "87e1794f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-03T14:17:43.786769Z",
     "iopub.status.busy": "2025-05-03T14:17:43.786463Z",
     "iopub.status.idle": "2025-05-04T00:58:13.196230Z",
     "shell.execute_reply": "2025-05-04T00:58:13.195326Z",
     "shell.execute_reply.started": "2025-05-03T14:17:43.786742Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/20: 100%|██████████| 80000/80000 [34:56<00:00, 38.15it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20, Train Loss: 0.6724, Train Acc: 0.7424, Val Loss: 0.6322, Val Acc: 0.7792\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/20: 100%|██████████| 80000/80000 [35:36<00:00, 37.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/20, Train Loss: 0.6282, Train Acc: 0.7821, Val Loss: 0.6232, Val Acc: 0.7894\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/20: 100%|██████████| 80000/80000 [35:49<00:00, 37.21it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/20, Train Loss: 0.6159, Train Acc: 0.7915, Val Loss: 0.6138, Val Acc: 0.7946\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/20: 100%|██████████| 80000/80000 [36:18<00:00, 36.73it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/20, Train Loss: 0.6083, Train Acc: 0.7973, Val Loss: 0.6131, Val Acc: 0.7972\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/20: 100%|██████████| 80000/80000 [36:17<00:00, 36.73it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/20, Train Loss: 0.6025, Train Acc: 0.8016, Val Loss: 0.6106, Val Acc: 0.7998\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6/20: 100%|██████████| 80000/80000 [35:52<00:00, 37.17it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6/20, Train Loss: 0.5977, Train Acc: 0.8052, Val Loss: 0.6122, Val Acc: 0.7998\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7/20: 100%|██████████| 80000/80000 [35:50<00:00, 37.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7/20, Train Loss: 0.5937, Train Acc: 0.8085, Val Loss: 0.6060, Val Acc: 0.8024\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8/20: 100%|██████████| 80000/80000 [36:22<00:00, 36.66it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8/20, Train Loss: 0.5904, Train Acc: 0.8112, Val Loss: 0.6117, Val Acc: 0.8033\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9/20: 100%|██████████| 80000/80000 [36:19<00:00, 36.70it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9/20, Train Loss: 0.5872, Train Acc: 0.8135, Val Loss: 0.6072, Val Acc: 0.8031\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10/20: 100%|██████████| 80000/80000 [36:14<00:00, 36.79it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/20, Train Loss: 0.5845, Train Acc: 0.8154, Val Loss: 0.6023, Val Acc: 0.8048\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 11/20: 100%|██████████| 80000/80000 [35:43<00:00, 37.32it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11/20, Train Loss: 0.5824, Train Acc: 0.8173, Val Loss: 0.6055, Val Acc: 0.8056\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 12/20: 100%|██████████| 80000/80000 [35:19<00:00, 37.75it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12/20, Train Loss: 0.5806, Train Acc: 0.8186, Val Loss: 0.6030, Val Acc: 0.8056\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 13/20: 100%|██████████| 80000/80000 [35:20<00:00, 37.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13/20, Train Loss: 0.5793, Train Acc: 0.8195, Val Loss: 0.6046, Val Acc: 0.8058\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 14/20: 100%|██████████| 80000/80000 [35:23<00:00, 37.68it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14/20, Train Loss: 0.5782, Train Acc: 0.8205, Val Loss: 0.6056, Val Acc: 0.8062\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 15/20: 100%|██████████| 80000/80000 [35:16<00:00, 37.79it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15/20, Train Loss: 0.5776, Train Acc: 0.8210, Val Loss: 0.6057, Val Acc: 0.8062\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 16/20: 100%|██████████| 80000/80000 [35:18<00:00, 37.77it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 16/20, Train Loss: 0.5772, Train Acc: 0.8215, Val Loss: 0.6057, Val Acc: 0.8062\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 17/20: 100%|██████████| 80000/80000 [35:23<00:00, 37.68it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 17/20, Train Loss: 0.5776, Train Acc: 0.8211, Val Loss: 0.6067, Val Acc: 0.8062\n",
      "Early stopping triggered after 17 epochs\n"
     ]
    }
   ],
   "source": [
    "# Optimizer and scheduler\n",
    "loss_fn = nn.CrossEntropyLoss(label_smoothing=0.1)\n",
    "optimizer = AdamW(model.parameters(), lr=1e-5, weight_decay=0.05)\n",
    "scheduler = CosineAnnealingLR(optimizer, T_max=15)\n",
    "\n",
    "# Training loop with early stopping\n",
    "num_epochs = 20\n",
    "patience = 7\n",
    "best_val_loss = float('inf')\n",
    "epochs_no_improve = 0\n",
    "\n",
    "model.train()\n",
    "for epoch in range(num_epochs):\n",
    "    total_train_loss = 0\n",
    "    total_train_correct = 0\n",
    "    total_train_samples = 0\n",
    "\n",
    "    # Training\n",
    "    for batch in tqdm(train_dataloader, desc=f\"Epoch {epoch + 1}/{num_epochs}\"):\n",
    "        input_ids, attention_mask, labels = batch\n",
    "        input_ids = input_ids.to(device)\n",
    "        attention_mask = attention_mask.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        logits = model(input_ids, mask=attention_mask)\n",
    "        loss = loss_fn(logits, labels)\n",
    "\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "        optimizer.step()\n",
    "\n",
    "        total_train_loss += loss.item()\n",
    "        preds = torch.argmax(logits, dim=-1)\n",
    "        total_train_correct += (preds == labels).sum().item()\n",
    "        total_train_samples += labels.size(0)\n",
    "\n",
    "    train_loss = total_train_loss / len(train_dataloader)\n",
    "    train_accuracy = total_train_correct / total_train_samples\n",
    "\n",
    "    # Validation\n",
    "    model.eval()\n",
    "    total_val_loss = 0\n",
    "    total_val_correct = 0\n",
    "    total_val_samples = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in val_dataloader:\n",
    "            input_ids, attention_mask, labels = batch\n",
    "            input_ids = input_ids.to(device)\n",
    "            attention_mask = attention_mask.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            logits = model(input_ids, mask=attention_mask)\n",
    "            loss = loss_fn(logits, labels)\n",
    "\n",
    "            total_val_loss += loss.item()\n",
    "            preds = torch.argmax(logits, dim=-1)\n",
    "            total_val_correct += (preds == labels).sum().item()\n",
    "            total_val_samples += labels.size(0)\n",
    "\n",
    "    val_loss = total_val_loss / len(val_dataloader)\n",
    "    val_accuracy = total_val_correct / total_val_samples\n",
    "\n",
    "    print(f\"Epoch {epoch + 1}/{num_epochs}, Train Loss: {train_loss:.4f}, Train Acc: {train_accuracy:.4f}, Val Loss: {val_loss:.4f}, Val Acc: {val_accuracy:.4f}\")\n",
    "\n",
    "    # Early stopping\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        epochs_no_improve = 0\n",
    "        torch.save(model.state_dict(), 'best_model.pt')\n",
    "    else:\n",
    "        epochs_no_improve += 1\n",
    "        if epochs_no_improve >= patience:\n",
    "            print(f\"Early stopping triggered after {epoch + 1} epochs\")\n",
    "            break\n",
    "\n",
    "    scheduler.step()\n",
    "    model.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d9666dc",
   "metadata": {},
   "source": [
    "### Training Results and Recommendations\n",
    "\n",
    "After training and testing the model on many examples, the model performed well and showed accurate classifications, which made me satisfied with the results. Early stopping was triggered after 17 epochs because the validation loss stopped improving, which helped prevent overfitting.\n",
    "\n",
    "If you want to make the model even better or ensure it completes all 20 epochs without overfitting, you can try these tips:\n",
    "- Reduce the dropout rate slightly.\n",
    "- Increase the patience parameter for early stopping.\n",
    "- Use more advanced learning rate scheduling.\n",
    "- Try data augmentation or regularization techniques.\n",
    "\n",
    "These adjustments can help the model train longer while still maintaining good generalization."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94b39f5f",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "aa4f71bc",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-04T00:58:13.197557Z",
     "iopub.status.busy": "2025-05-04T00:58:13.197181Z",
     "iopub.status.idle": "2025-05-04T01:00:07.618750Z",
     "shell.execute_reply": "2025-05-04T01:00:07.618011Z",
     "shell.execute_reply.started": "2025-05-04T00:58:13.197540Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_31/2838731559.py:2: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load('best_model.pt'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy: 0.8048\n",
      "Validation F1-Score: 0.8047\n",
      "Test Sentence: I love this beautiful day!\n",
      "Predicted Class: Positive\n"
     ]
    }
   ],
   "source": [
    "# Load best model\n",
    "model.load_state_dict(torch.load('best_model.pt'))\n",
    "model.eval()\n",
    "\n",
    "# Evaluate on validation set\n",
    "all_preds = []\n",
    "all_labels = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch in val_dataloader:\n",
    "        input_ids, attention_mask, labels = batch\n",
    "        input_ids = input_ids.to(device)\n",
    "        attention_mask = attention_mask.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        logits = model(input_ids, mask=attention_mask)\n",
    "        preds = torch.argmax(logits, dim=-1)\n",
    "\n",
    "        all_preds.extend(preds.cpu().numpy())\n",
    "        all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "# Calculate metrics\n",
    "accuracy = sum(p == l for p, l in zip(all_preds, all_labels)) / len(all_labels)\n",
    "f1 = f1_score(all_labels, all_preds, average='weighted')\n",
    "\n",
    "print(f\"Validation Accuracy: {accuracy:.4f}\")\n",
    "print(f\"Validation F1-Score: {f1:.4f}\")\n",
    "\n",
    "# Test on a single sentence\n",
    "test_sentence = \"I love this beautiful day!\"\n",
    "test_inputs = tokenizer(test_sentence, return_tensors=\"pt\", padding=True, truncation=True, max_length=64)\n",
    "input_ids = test_inputs['input_ids'].to(device)\n",
    "attention_mask = test_inputs['attention_mask'].to(device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    logits = model(input_ids, mask=attention_mask)\n",
    "    pred = torch.argmax(logits, dim=-1).item()\n",
    "\n",
    "class_names = {0: \"Negative\", 1: \"Neutral\", 2: \"Positive\"}\n",
    "print(f\"Test Sentence: {test_sentence}\")\n",
    "print(f\"Predicted Class: {class_names[pred]}\")"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [],
   "dockerImageVersionId": 31012,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
